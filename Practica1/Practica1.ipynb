{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Pablo Marcos y Dionisio Perez\n",
    "\n",
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "import collections\n",
    "from abc import ABCMeta,abstractmethod\n",
    "from scipy.stats import norm\n",
    "\n",
    "#from sklearn import datasets\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Datos(object):\n",
    "    \"\"\"Clase para leer y almacenar los datos de los ficheros .data proporcionados\n",
    "\n",
    "    Attributes:\n",
    "        ndatos (int): Numero de entradas de nuestro conjunto de datos\n",
    "        nAtributos (int): Numero de atributos de cada dato\n",
    "        nombreAtributos (list): Lista con los nombres de los atributos\n",
    "        tipoAtributos (list): Lista con string representando el tipo de cada atributo\n",
    "        nominalAtributos (list): Lista con True en las posiciones de los atributos nominales\n",
    "        diccionarios (list): Lista de diccionarios con el valor de cada uno de los atributos nominales\n",
    "        datos (numpy.ndarray) : Matrix ndatosxnAtributos con los datos recolectados y los atributos\n",
    "            nominales traducidos.\n",
    "    \"\"\"\n",
    "\n",
    "    TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "    def __init__(self, nombreFichero, cast=None):\n",
    "        \"\"\"Constructor de la clase Datos\n",
    "\n",
    "        Args:\n",
    "            nombreFichero (str): path del fichero de datos a cargar\n",
    "            cast (np.dtype, opcional) : Si se especifica la matriz de datos se\n",
    "                casteara al tipo especificado, en otro caso si todos los atributos\n",
    "                son nominales se almacenaran en tipo entero y si hay algun dato\n",
    "                continuo en tipo float.\n",
    "        \"\"\"\n",
    "\n",
    "        # Abrimos el fichero y procesamos la cabecera\n",
    "        with open(nombreFichero) as f:\n",
    "\n",
    "            # Guardamos el numero de datos\n",
    "            self.nDatos = int(f.readline())\n",
    "\n",
    "            # Guardamos la lista de nombres de atributos\n",
    "            self.nombreAtributos = f.readline().replace('\\n','').split(\",\")\n",
    "\n",
    "            # Guardamos la lista de atributos\n",
    "            self.tipoAtributos = f.readline().replace('\\n','').split(\",\")\n",
    "\n",
    "            # Numero de atributos\n",
    "            self.nAtributos = len(self.tipoAtributos)\n",
    "\n",
    "            # Comprobacion atributos\n",
    "            if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "                raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "            # Guardamos True en las posiciones de atributos nominales\n",
    "            self.nominalAtributos = [atr == 'Nominal' for atr in self.tipoAtributos]\n",
    "\n",
    "        # Leemos los datos de numpy en formate string para los datos nominales\n",
    "        datosNominales = np.genfromtxt(nombreFichero, dtype='S', skip_header=3, delimiter=',')\n",
    "\n",
    "        # Inicializamos los diccionarios con los distintos valores de los atributos\n",
    "        self._inicializarDiccionarios(datosNominales)\n",
    "\n",
    "        # Transformamos los datos nominales en datos numericos empleando los diccionarios\n",
    "        for i, nominal in enumerate(self.nominalAtributos):\n",
    "            if nominal:\n",
    "                datosNominales[:,i] = np.vectorize(self.diccionarios[i].get)(datosNominales[:,i])\n",
    "\n",
    "        # Convertimos la matriz a tipo numerico, en caso de no especificarse\n",
    "        # Si todos los atributos son nominales usamos el tipo np.int para ahorrar espacio\n",
    "        # Si hay datos continuos lo guardamos en tipo np.float\n",
    "        if cast == None: cast = np.int if all(self.nominalAtributos) else np.float\n",
    "        self.datos = datosNominales.astype(cast)\n",
    "\n",
    "        # Convertimos los nombres nominales a string en vez de dejarlos en bytes\n",
    "        diccionarios_aux = []\n",
    "        for d in self.diccionarios:\n",
    "            aux = {}\n",
    "            for k in d: aux[k.decode('utf-8')] = d[k]\n",
    "            diccionarios_aux.append(aux)\n",
    "\n",
    "        self.diccionarios = diccionarios_aux\n",
    "\n",
    "    def _inicializarDiccionarios(self, datos):\n",
    "        \"\"\"Funcion interna para inicializar los diccionarios buscando todos\n",
    "            los valores que toman los atributos en la matriz de datos\"\"\"\n",
    "\n",
    "        self.diccionarios = []\n",
    "\n",
    "        for i, nominal in enumerate(self.nominalAtributos):\n",
    "\n",
    "            if not nominal: # Incluimos diccionarios vacios en los datos no nominales\n",
    "                self.diccionarios.append({})\n",
    "            else:\n",
    "                # Buscamos todos los valores distintos por atributo y creamos el diccionario\n",
    "                values = np.unique(datos[:,i])\n",
    "                values.sort()\n",
    "                self.diccionarios.append({k: v for v, k in enumerate(values)})\n",
    "\n",
    "    def extraeDatos(self, idx):\n",
    "        return self.datos[idx]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.extraeDatos(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nDatos\n",
    "    \n",
    "    def __yield__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self.datos[i]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion:\n",
    "  \n",
    "    def __init__(self, train=[], test=[]):\n",
    "        self.indicesTrain= train\n",
    "        self.indicesTest= test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Particion:\\nTrain: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  \n",
    "      # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "  \n",
    "    def __init__(self, nombre=\"null\"):\n",
    "        self.nombreEstrategia=nombre\n",
    "        self.numeroParticiones=0\n",
    "        self.particiones=[]\n",
    "    \n",
    "    def __call__(self, datos):\n",
    "        return self.creaParticiones(datos)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for part in self.particiones:\n",
    "            yield part\n",
    "  \n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta  \n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \"\"\"Crea particiones segun el metodo tradicional \n",
    "    de division de los datos segun el porcentaje deseado.\"\"\"\n",
    "    \n",
    "    def __init__(self,porcentaje=.75):\n",
    "\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion Simple con {}\\% de entrenamiento\".format(100*porcentaje))\n",
    "\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):    \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "\n",
    "        # Generamos una permutacion de los indices\n",
    "        indices = np.arange(datos.nDatos)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # Separamos en base al porcentaje necesario\n",
    "        l = int(datos.nDatos*self.porcentaje)\n",
    "        self.particiones = [Particion(indices[:l], indices[l:])]\n",
    "\n",
    "        return self.particiones\n",
    "    \n",
    "      \n",
    "#####################################################################################################      \n",
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion Cruzada con {} particiones\".format(k))\n",
    "  \n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones\n",
    "  # y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "    def creaParticiones(self,datos,seed=None):   \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        # Tam de cada bloque\n",
    "        l = int(datos.nDatos/self.k)\n",
    "        \n",
    "        # Generamos una permutacion de los indices\n",
    "        indices = np.arange(datos.nDatos)\n",
    "        np.random.shuffle(indices)\n",
    "        self.particiones = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.k):\n",
    "\n",
    "            train = np.delete(indices, range(i*l,(i+1)*l))\n",
    "            test =  indices[i*l:(i+1)*l-1]\n",
    "            self.particiones.append(Particion(train, test))\n",
    "                                    \n",
    "        return self.particiones\n",
    "            \n",
    "        \n",
    "    \n",
    "#####################################################################################################\n",
    "\n",
    "class ValidacionBootstrap(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        super().__init__(\"Validacion Bootstrap con {} particiones\".format(n))\n",
    "        self.n = n\n",
    "\n",
    "  # Crea particiones segun el metodo de boostrap\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "    def creaParticiones(self,datos,seed=None):    \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = self.n\n",
    "\n",
    "        # Generamos una permutacion de los indices\n",
    "        indices = np.arange(datos.nDatos)\n",
    "        self.particiones = []\n",
    "\n",
    "        for i in range(self.n):\n",
    "\n",
    "            # Generamos numeros aleatorios con repeticion\n",
    "            aleatorios = np.random.randint(0, datos.nDatos, datos.nDatos)\n",
    "            # Nos quedamos los ejemplos de los indices\n",
    "            train = indices[aleatorios]\n",
    "            # Obtenemos los indices que han sido excluidos\n",
    "            excluidos = [i not in aleatorios for i in indices] \n",
    "            \n",
    "            # El conjunto de indices esta formado por los indices excluidos\n",
    "            test = indices[excluidos]\n",
    "\n",
    "            self.particiones.append(Particion(train, test))\n",
    "\n",
    "        return self.particiones\n",
    "    \n",
    "#####################################################################################################\n",
    "\n",
    "    \n",
    "class ValidacionSimpleScikit(EstrategiaParticionado):\n",
    "    \"\"\"Crea particiones segun el metodo tradicional \n",
    "    de division de los datos segun el porcentaje deseado.\"\"\"\n",
    "    \n",
    "    def __init__(self,porcentaje=.75):\n",
    "\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion Simple Scikit con {}\\% de entrenamiento\".format(100*porcentaje))\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None): \n",
    "        \n",
    "        #Primero encriptamos los atributos\n",
    "        encAtributos = preprocessing.OneHotEncoder(categorical_features=datos.nominalAtributos[:-1], sparse=False)\n",
    "        \n",
    "        # Guardamos la matriz de atributos codificada\n",
    "        X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "        # Guardamos la clase de cada patron\n",
    "        Y =dataset.datos[:,-1] \n",
    "\n",
    "        # Creamos Train y Test para atributos y clases (clases tb?)\n",
    "        XTrain, XTest = train_test_split(X, train_size = self.porcentaje, shuffle=True)\n",
    "        YTrain, YTest = train_test_split(Y, train_size = self.porcentaje, shuffle=True) #?\n",
    "        \n",
    "        self.particiones.append(Particion(XTrain, XTest))\n",
    "        self.particiones.append(Particion(YTrain, YTest))\n",
    "        \n",
    "        return self.particiones\n",
    "    \n",
    "#####################################################################################################\n",
    "\n",
    "    \n",
    "class ValidacionCruzadaScikit(EstrategiaParticionado):\n",
    "    \"\"\"Crea particiones segun el metodo tradicional \n",
    "    de division de los datos segun el porcentaje deseado.\"\"\"\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion Cruzada Scikit con {} particiones\".format(k))\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None): \n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        self.particiones = []\n",
    "            \n",
    "        #Primero encriptamos los atributos\n",
    "        encAtributos = preprocessing.OneHotEncoder(categorical_features=datos.nominalAtributos[:-1], sparse=False)\n",
    "        \n",
    "        # Guardamos la matriz de atributos codificada\n",
    "        X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "        # Guardamos la clase de cada patron\n",
    "        Y =dataset.datos[:,-1]    \n",
    "\n",
    "        # Creamos particiones\n",
    "        kf = KFold(n_splits=self.k, shuffle=True)\n",
    "        for train, test in kf.split(X, Y):\n",
    "            XTrain, XTest = X[train], X[test]\n",
    "            YTrain, YTest = Y[train], Y[test]\n",
    "            self.particiones.append(Particion(XTrain, XTest))\n",
    "            self.particiones.append(Particion(YTrain, YTest))\n",
    "        \n",
    "        return self.particiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prueba de los metodos de particiones de la semana1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    dataset = Datos('../ConjuntosDatos/balloons.data')\n",
    "    \n",
    "    \n",
    "    # Creamos una particion con validacion simple\n",
    "    validacion1 = ValidacionSimple(0.8)\n",
    "    particion1 = validacion1(dataset)\n",
    "    \n",
    "    # Creamos una particion con validacion cruzada\n",
    "    k=4\n",
    "    validacion2 = ValidacionCruzada(k)\n",
    "    particion2 = validacion2(dataset)\n",
    "    \n",
    "    # Creamos una particion usando Validacion Bootstrap\n",
    "    n=10\n",
    "    validacion3 = ValidacionBootstrap(n)\n",
    "    particion3 = validacion3(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion\n",
    "    # de variables discretas\n",
    "    def entrenamiento(self,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        pass\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test\n",
    "        pass\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "\n",
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "\n",
    "    def entrenamiento(self, datos, indices): \n",
    "\n",
    "        # Numero de atributos discretos\n",
    "        self.nAtributosDiscretos = sum(datos.nominalAtributos) - 1\n",
    "        self.nAtributos = len(datos.diccionarios) - 1\n",
    "        self.nAtributosContinuos = self.nAtributos - self.nAtributosDiscretos       \n",
    "\n",
    "        # Numero de clases (longitud del diccionario del campo clase)\n",
    "        self.nClases = len(datos.diccionarios[-1])\n",
    "        \n",
    "        datosEntrenamiento = datos[indices]\n",
    "        arrayClases = datosEntrenamiento[:,-1]\n",
    "        nEntrenamiento = len(indices)\n",
    "        \n",
    "        # Maximo tam de posibles valores de atributos nominales\n",
    "        self.vAtributos = max([len(d) for d in datos.diccionarios[:-1]])\n",
    "        \n",
    "        # Tablas de probabilidad a posteriori\n",
    "        # Matrix con (indice Atributo Discreto, valor Atributo, Clase)\n",
    "        self.posteriori = np.zeros((self.nAtributosDiscretos,self.vAtributos, self.nClases))\n",
    "        self.gaussianas =  np.zeros((self.nAtributosContinuos,self.nClases, 2))\n",
    "        # Mapeo entre indices reales y los indices de nuestras tablas\n",
    "        self.indicesDiscretos = {}\n",
    "        self.indicesContinuos = {}\n",
    "        d_index = 0 # Cuenta de indices empleados discretos\n",
    "        c_index = 0 # y continuos\n",
    "        \n",
    "        self.priori = np.empty(self.nClases)\n",
    "        \n",
    "        for c in range(self.nClases):\n",
    "            self.priori[c] = len(np.where(arrayClases == c)[0])/ nEntrenamiento\n",
    "\n",
    "        \n",
    "               \n",
    "        # Iteramos sobre los datos de entrenamiento (sin contar la clase)\n",
    "        for k, discreto in enumerate(datos.nominalAtributos[:-1]):\n",
    "            if discreto:\n",
    "                self.indicesDiscretos[k] = d_index\n",
    "                self._entrena_discreto(d_index, datosEntrenamiento[:,k], arrayClases, datos.diccionarios[k], nEntrenamiento)\n",
    "                d_index += 1\n",
    "            else:\n",
    "                self.indicesContinuos[k] = c_index\n",
    "                self._entrena_continuo(c_index, datosEntrenamiento[:,k], arrayClases)                \n",
    "                c_index += 1\n",
    "\n",
    "                \n",
    "    def _entrena_discreto(self, k, datos, clase, diccionario, n):\n",
    "        \n",
    "        # Calculamos los conteos de atributos de cada clase\n",
    "        repeticiones = np.empty(self.nClases, dtype=object)\n",
    "        for c in range(self.nClases):\n",
    "            repeticiones[c] = collections.Counter(datos[clase == c])\n",
    "\n",
    "            \n",
    "        # Correccion de laplace\n",
    "        laplace = any(np.array([len(r) for r in repeticiones]) != len(diccionario))\n",
    "        l= 1 if laplace else 0\n",
    "        \n",
    "        for c in range(self.nClases):\n",
    "            for v in diccionario:\n",
    "                atr = diccionario[v]\n",
    "                self.posteriori[k][atr][c] = (l + repeticiones[c][atr]) / (n+l)\n",
    "                \n",
    "    def _entrena_continuo(self, k, datos, clase):\n",
    "        \n",
    "        for c in range(self.nClases):\n",
    "            data = datos[clase == c]\n",
    "            self.gaussianas[k,c,0] = np.mean(data)\n",
    "            self.gaussianas[k,c,1] = np.std(data)\n",
    "            \n",
    "    def _probabilidadClase(self, dato, clase, datos):\n",
    "\n",
    "        prob = self.priori[clase]\n",
    "        print(\" ~En la clase: \",clase, \" con a priori: [\", prob, \"]\")\n",
    "        \n",
    "        for atr, nominal in enumerate(datos.nominalAtributos[:-1]): \n",
    "            if nominal:\n",
    "                print(\"Es nominal: \", nominal)\n",
    "                index = self.indicesDiscretos[atr]\n",
    "                print(\"Indice atributo\", index, \"valor\", dato[atr], \"clase\", clase)\n",
    "                prob *= self.posteriori[index,dato[atr],clase]\n",
    "                print(\"posteriori\", self.posteriori[index,dato[atr],clase])\n",
    "            else:\n",
    "                index = self.indicesContinuos[atr]\n",
    "                prob *= norm.pdf(dato[atr], self.gaussianas[index,clase,0], \n",
    "                                self.gaussianas[index,clase,1])\n",
    "            \n",
    "        return prob\n",
    "\n",
    "            \n",
    "\n",
    "    def clasifica(self,datos, indices):\n",
    "        \n",
    "        \n",
    "        clasificacion = np.full(len(indices), 99)\n",
    "        probabilidades = np.zeros(self.nClases)\n",
    "        \n",
    "        \n",
    "        for j, dato in enumerate(datos[indices]):\n",
    "            print(\"Clasificando dato: \", dato)\n",
    "            for clase in range(self.nClases):\n",
    "                #print(\"Clase\", clase)\n",
    "                probabilidades[clase] = self._probabilidadClase(dato, clase, datos)\n",
    "                #print(\"La probs\", probabilidades)\n",
    "            print(\"Las probabilidades: \", probabilidades)\n",
    "            print(\"La mayor es: \", np.argmax(probabilidades))\n",
    "            clasificacion[j] = np.argmax(probabilidades)\n",
    "            print()\n",
    "            \n",
    "        print(\"CLASIFICACION: \",clasificacion)  \n",
    "        return clasificacion\n",
    "\n",
    "    \n",
    "    ##############################################################################\n",
    "\n",
    "class ScikitNaiveBayes(Clasificador):\n",
    "    \n",
    "    def entrenamiento(self, datos, indices):\n",
    "          \n",
    "        # Calculamos las posteriori de los discretos\n",
    "        self.NB = nb.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "        # Calculamos NaiveBayes\n",
    "        self.NB.fit(indices[0], indices[1])\n",
    "        # indices[0] contiene la particion de datos\n",
    "        # indices[1] contiene la particion de clases (?) \n",
    "        # y en el caso de val.cruzada?\n",
    "\n",
    "    def clasifica(self, datos, indices):\n",
    "        \n",
    "        print(self.NB.predict(indices[0]))     \n",
    "                \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3.5/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('../ConjuntosDatos/balloons.data')\n",
    "\n",
    "# Creamos particion\n",
    "#validacion1 = ValidacionSimple(0.8)\n",
    "#particion1 = validacion1(dataset)\n",
    "\n",
    "# Creamos clasificador Naive-Bayes\n",
    "#c = ClasificadorNaiveBayes()\n",
    "#c.entrenamiento(dataset, particion1[0].indicesTrain)\n",
    "#clasificacion = c.clasifica(dataset, particion1[0].indicesTest)\n",
    "#print(dataset[particion1[0].indicesTest])\n",
    "#clasificacion = c.clasifica(dataset, range(len(dataset)))\n",
    "\n",
    "# Particion por validacion simple scikit\n",
    "validacion1 = ValidacionSimpleScikit(0.8)\n",
    "particion1 = validacion1(dataset)\n",
    "#Particion por validacion cruzada scikit\n",
    "validacion2 = ValidacionCruzadaScikit(4)\n",
    "particion2 = validacion2(dataset)\n",
    "\n",
    "c = ScikitNaiveBayes()\n",
    "c.entrenamiento(dataset, [particion2[0].indicesTrain, particion2[1].indicesTrain])\n",
    "c.clasifica(dataset, [particion2[0].indicesTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ScikitNaiveBayes' object has no attribute 'posteriori'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-098c9372cd59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(c.gaussianas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ScikitNaiveBayes' object has no attribute 'posteriori'"
     ]
    }
   ],
   "source": [
    "#print(c.gaussianas)\n",
    "print(c.posteriori)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
